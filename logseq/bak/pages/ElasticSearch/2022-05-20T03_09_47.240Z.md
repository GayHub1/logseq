# Elasticsearch 核心技术与实战

ElasticSearch起源于Lucene的一款搜索引擎.具有**易扩展**,**高可用**的特点.支持不同的节点类型.支持多种语言类库接入.常用于**实时**搜索.分析.
## 安装
### Elasticsearch 安装

1. 从[官网](https://www.elastic.co/downloads/elasticsearch)下载对应系统安装包

2. 修改配置(建议jvm的 Xmx与Xms内存设置一样且不超过机器内存的一半或30G)

3. 执行`bin/elasticsearch` (or `bin\elasticsearch.bat` )文件启动

4. 留意命令行输出 将显示的密码和注册令牌(token)保存起来

 ```ssh
 ------------------------------------------------------------------------------------------------------------------------
 -> Elasticsearch security features have been automatically configured!
 -> Authentication is enabled and cluster connections are encrypted.
 
 ->  Password for the elastic user (reset with `bin/elasticsearch-reset-password -u elastic`):
   S1t0X*uv0ExZLS-2hRnd
 
 ->  HTTP CA certificate SHA-256 fingerprint:
   7ab1e8853b0cf3b49c9d9d4f1d7173b00082a7f38517bc0bbcbe7c6f0f1bbf45
 
 ->  Configure Kibana to use this cluster:
 * Run Kibana and click the configuration link in the terminal when Kibana starts.
 * Copy the following enrollment token and paste it into Kibana in your browser (valid for the next 30 minutes):
   eyJ2ZXIiOiI4LjEuMyIsImFkciI6WyIxNzIuMjQuMTYwLjE6OTIwMCIsIjE5Mi4xNjguMjAwLjIwMjo5MjAwIiwiMTcyLjI4LjY0LjE6OTIwMCJdLCJmZ3IiOiI3YWIxZTg4NTNiMGNmM2I0OWM5ZDlkNGYxZDcxNzNiMDAwODJhN2YzODUxN2JjMGJiY2JlN2M2ZjBmMWJiZjQ1Iiwia2V5IjoieXlEQ2FZQUJjUHBKMXFfTkF4bGM6QXc5MW5Gc01TNGV5aFdzd1A0UzhJQSJ9
 
 ->  Configure other nodes to join this cluster:
 * On this node:
   - Create an enrollment token with `bin/elasticsearch-create-enrollment-token -s node`.
   - Uncomment the transport.host setting at the end of config/elasticsearch.yml.
   - Restart Elasticsearch.
 * On other nodes:
   - Start Elasticsearch with `bin/elasticsearch --enrollment-token <token>`, using the enrollment token that you generated.
 ------------------------------------------------------------------------------------------------------------------------
 
 ```

 5. 访问[https://127.0.0.1:9200](https://127.0.0.1:9200/)输入 用户名 elastic 与上文密码即可看见已成功启动
### Kibana安装

1. 从[官网](https://www.elastic.co/cn/downloads/kibana)下载对应系统安装包

2. 执行`bin/kibana` (or `bin\kibana.bat` on Windows)文件启动

3. 访问 http://localhost:5601 输入上文的ElasticSearchToken即可

 如果token过期在es中运行以下命令重新创建token

 ```ssh
 elasticsearch-create-enrollment-token -s kibana --url "https://127.0.0.1:9200"
 ```
### Docker-compose安装

最新8.1.3版本

ENV文件

```.env
# Password for the 'elastic' user (at least 6 characters)
ELASTIC_PASSWORD=elastic

# Password for the 'kibana_system' user (at least 6 characters)
KIBANA_PASSWORD=elastic

# Version of Elastic products
STACK_VERSION=8.1.3

# Set the cluster name
CLUSTER_NAME=docker-cluster

# Set to 'basic' or 'trial' to automatically start the 30-day trial
LICENSE=basic
#LICENSE=trial

# Port to expose Elasticsearch HTTP API to the host
ES_PORT=9200
#ES_PORT=127.0.0.1:9200

# Port to expose Kibana to the host
KIBANA_PORT=5601
#KIBANA_PORT=80

# Increase or decrease based on the available host memory (in bytes)
MEM_LIMIT=1073741824

# Project namespace (defaults to the current folder name if not set)
#COMPOSE_PROJECT_NAME=myproject
```

docker-compose.yaml文件

```
version: "2.2"

services:
setup:
  image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
  volumes:
    - certs:/usr/share/elasticsearch/config/certs
  user: "0"
  command: >
    bash -c '
      if [ x${ELASTIC_PASSWORD} == x ]; then
        echo "Set the ELASTIC_PASSWORD environment variable in the .env file";
        exit 1;
      elif [ x${KIBANA_PASSWORD} == x ]; then
        echo "Set the KIBANA_PASSWORD environment variable in the .env file";
        exit 1;
      fi;
      if [ ! -f config/certs/ca.zip ]; then
        echo "Creating CA";
        bin/elasticsearch-certutil ca --silent --pem -out config/certs/ca.zip;
        unzip config/certs/ca.zip -d config/certs;
      fi;
      if [ ! -f config/certs/certs.zip ]; then
        echo "Creating certs";
        echo -ne \
        "instances:\n"\
        "  - name: es01\n"\
        "    dns:\n"\
        "      - es01\n"\
        "      - localhost\n"\
        "    ip:\n"\
        "      - 127.0.0.1\n"\
        "  - name: es02\n"\
        "    dns:\n"\
        "      - es02\n"\
        "      - localhost\n"\
        "    ip:\n"\
        "      - 127.0.0.1\n"\
        "  - name: es03\n"\
        "    dns:\n"\
        "      - es03\n"\
        "      - localhost\n"\
        "    ip:\n"\
        "      - 127.0.0.1\n"\
        > config/certs/instances.yml;
        bin/elasticsearch-certutil cert --silent --pem -out config/certs/certs.zip --in config/certs/instances.yml --ca-cert config/certs/ca/ca.crt --ca-key config/certs/ca/ca.key;
        unzip config/certs/certs.zip -d config/certs;
      fi;
      echo "Setting file permissions"
      chown -R root:root config/certs;
      find . -type d -exec chmod 750 \{\} \;;
      find . -type f -exec chmod 640 \{\} \;;
      echo "Waiting for Elasticsearch availability";
      until curl -s --cacert config/certs/ca/ca.crt https://es01:9200 | grep -q "missing authentication credentials"; do sleep 30; done;
      echo "Setting kibana_system password";
      until curl -s -X POST --cacert config/certs/ca/ca.crt -u elastic:${ELASTIC_PASSWORD} -H "Content-Type: application/json" https://es01:9200/_security/user/kibana_system/_password -d "{\"password\":\"${KIBANA_PASSWORD}\"}" | grep -q "^{}"; do sleep 10; done;
      echo "All done!";
    '
  healthcheck:
    test: ["CMD-SHELL", "[ -f config/certs/es01/es01.crt ]"]
    interval: 1s
    timeout: 5s
    retries: 120

es01:
  depends_on:
    setup:
      condition: service_healthy
  image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
  volumes:
    - certs:/usr/share/elasticsearch/config/certs
    - esdata01:/usr/share/elasticsearch/data
  ports:
    - ${ES_PORT}:9200
  environment:
    - node.name=es01
    - cluster.name=${CLUSTER_NAME}
    - cluster.initial_master_nodes=es01,es02,es03
    - discovery.seed_hosts=es02,es03
    - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
    - bootstrap.memory_lock=true
    - xpack.security.enabled=true
    - xpack.security.http.ssl.enabled=true
    - xpack.security.http.ssl.key=certs/es01/es01.key
    - xpack.security.http.ssl.certificate=certs/es01/es01.crt
    - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
    - xpack.security.http.ssl.verification_mode=certificate
    - xpack.security.transport.ssl.enabled=true
    - xpack.security.transport.ssl.key=certs/es01/es01.key
    - xpack.security.transport.ssl.certificate=certs/es01/es01.crt
    - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
    - xpack.security.transport.ssl.verification_mode=certificate
    - xpack.license.self_generated.type=${LICENSE}
  mem_limit: ${MEM_LIMIT}
  ulimits:
    memlock:
      soft: -1
      hard: -1
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
      ]
    interval: 10s
    timeout: 10s
    retries: 120

es02:
  depends_on:
    - es01
  image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
  volumes:
    - certs:/usr/share/elasticsearch/config/certs
    - esdata02:/usr/share/elasticsearch/data
  environment:
    - node.name=es02
    - cluster.name=${CLUSTER_NAME}
    - cluster.initial_master_nodes=es01,es02,es03
    - discovery.seed_hosts=es01,es03
    - bootstrap.memory_lock=true
    - xpack.security.enabled=true
    - xpack.security.http.ssl.enabled=true
    - xpack.security.http.ssl.key=certs/es02/es02.key
    - xpack.security.http.ssl.certificate=certs/es02/es02.crt
    - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
    - xpack.security.http.ssl.verification_mode=certificate
    - xpack.security.transport.ssl.enabled=true
    - xpack.security.transport.ssl.key=certs/es02/es02.key
    - xpack.security.transport.ssl.certificate=certs/es02/es02.crt
    - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
    - xpack.security.transport.ssl.verification_mode=certificate
    - xpack.license.self_generated.type=${LICENSE}
  mem_limit: ${MEM_LIMIT}
  ulimits:
    memlock:
      soft: -1
      hard: -1
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
      ]
    interval: 10s
    timeout: 10s
    retries: 120

es03:
  depends_on:
    - es02
  image: docker.elastic.co/elasticsearch/elasticsearch:${STACK_VERSION}
  volumes:
    - certs:/usr/share/elasticsearch/config/certs
    - esdata03:/usr/share/elasticsearch/data
  environment:
    - node.name=es03
    - cluster.name=${CLUSTER_NAME}
    - cluster.initial_master_nodes=es01,es02,es03
    - discovery.seed_hosts=es01,es02
    - bootstrap.memory_lock=true
    - xpack.security.enabled=true
    - xpack.security.http.ssl.enabled=true
    - xpack.security.http.ssl.key=certs/es03/es03.key
    - xpack.security.http.ssl.certificate=certs/es03/es03.crt
    - xpack.security.http.ssl.certificate_authorities=certs/ca/ca.crt
    - xpack.security.http.ssl.verification_mode=certificate
    - xpack.security.transport.ssl.enabled=true
    - xpack.security.transport.ssl.key=certs/es03/es03.key
    - xpack.security.transport.ssl.certificate=certs/es03/es03.crt
    - xpack.security.transport.ssl.certificate_authorities=certs/ca/ca.crt
    - xpack.security.transport.ssl.verification_mode=certificate
    - xpack.license.self_generated.type=${LICENSE}
  mem_limit: ${MEM_LIMIT}
  ulimits:
    memlock:
      soft: -1
      hard: -1
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "curl -s --cacert config/certs/ca/ca.crt https://localhost:9200 | grep -q 'missing authentication credentials'",
      ]
    interval: 10s
    timeout: 10s
    retries: 120

kibana:
  depends_on:
    es01:
      condition: service_healthy
    es02:
      condition: service_healthy
    es03:
      condition: service_healthy
  image: docker.elastic.co/kibana/kibana:${STACK_VERSION}
  volumes:
    - certs:/usr/share/kibana/config/certs
    - kibanadata:/usr/share/kibana/data
  ports:
    - ${KIBANA_PORT}:5601
  environment:
    - SERVERNAME=kibana
    - ELASTICSEARCH_HOSTS=https://es01:9200
    - ELASTICSEARCH_USERNAME=kibana_system
    - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
    - ELASTICSEARCH_SSL_CERTIFICATEAUTHORITIES=config/certs/ca/ca.crt
  mem_limit: ${MEM_LIMIT}
  healthcheck:
    test:
      [
        "CMD-SHELL",
        "curl -s -I http://localhost:5601 | grep -q 'HTTP/1.1 302 Found'",
      ]
    interval: 10s
    timeout: 10s
    retries: 120

volumes:
certs:
  driver: local
esdata01:
  driver: local
esdata02:
  driver: local
esdata03:
  driver: local
kibanadata:
  driver: local
```

6.4.0版本

```yaml
version: '3'
services:
elasticsearch:
  image: elasticsearch:6.4.0
  container_name: elasticsearch
  environment:
    - "cluster.name=elasticsearch" #设置集群名称为elasticsearch
    - "discovery.type=single-node" #以单一节点模式启动
    - "ES_JAVA_OPTS=-Xms512m -Xmx512m" #设置使用jvm内存大小
  volumes:
    - ./ELKConfig/elasticsearch/plugins:/usr/share/elasticsearch/plugins #插件文件挂载
    - ./ELKConfig/elasticsearch/data:/usr/share/elasticsearch/data #数据文件挂载
  ports:
    - 9200:9200
    - 9300:9300
kibana:
  image: kibana:6.4.0
  container_name: kibana
  links:
    - elasticsearch:es #可以用es这个域名访问elasticsearch服务
  depends_on:
    - elasticsearch #kibana在elasticsearch启动之后再启动
  environment:
    - "elasticsearch.hosts=http://es:9200" #设置访问elasticsearch的地址
  ports:
    - 5601:5601
logstash:
  image: logstash:6.4.0
  container_name: logstash
  volumes:
    - ./ELKConfig/logstash/logstash.conf:/usr/share/logstash/pipeline/logstash.conf #挂载logstash的配置文件
    - ./ELKConfig/logstash/movies.csv:/usr/share/logstash/data/movies.csv #挂载logstash的数据文件
  depends_on:
    - elasticsearch #kibana在elasticsearch启动之后再启动
  links:
    - elasticsearch:es #可以用es这个域名访问elasticsearch服务
  ports:
    - 4560:4560
# 配置采集服务
skywalking-oap:
  image: apache/skywalking-oap-server:8.7.0-es6
  container_name: skywalking-oap
  restart: always
  depends_on:
    - elasticsearch
  ports:
    - 8008:11800
    #- 8009:12800
  environment:
    TZ: Asia/Shanghai
    SW_STORAGE: elasticsearch
    SW_STORAGE_ES_CLUSTER_NODES: elasticsearch:9200
# 配置后台管理界面
skywalking-ui:
  image: apache/skywalking-ui:8.7.0
  container_name: skywalking-ui
  depends_on:
    - skywalking-oap
  restart: always
  ports:
    - 8010:8080
  environment:
    SW_OAP_ADDRESS: http://skywalking-oap:12800
    SW_TIMEOUT: 20000
```
### 报错

运行报错

```log
container for service "es02" is unhealthy
```

查看日志 

```
 max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
```

解决办法

**Linux**

将 `vm.max_map_count = 262144` 添加到 `/etc/sysctl.conf`

**Windows**

打开 powershell 运行

```
wsl -d docker-desktop
```

然后

```
sysctl -w vm.max_map_count=262144
```
### 安装与查看插件

查看插件

```
.\bin\elasticsearch-plugin list
```

安装插件

```
.\bin\elasticsearch-plugin install analysis-icu
```
- Kibana 相关插件 https://www.elastic.co/guide/en/kibana/current/known-plugins.html
## 基本概念

与关系型数据库简单对比

| RDBMS  | Elasticsearch |
| ------ | ------------- |
| Table  | Index(Type)   |
| Row    | Document      |
| Column | Filed         |
| Schema | Mapping       |
| SQL    | DSL           |
### 文档
- Elasticsearch 是面向文档的，文档是所有可搜索数据的最小单位
- 文档会被序列化成JSON格式，保存在Elasticsearch中
- 每个文档都有一个Unique ID
### 索引
- Index - 索引是文档的容器，是一类文档的结合
	- Index体现了逻辑空间的概念：每个索引都有自己的Mapping定义，用于定义包含的文档的字段名和字段类型
	- Shard体现了物理空间的概念：索引中的数据分散在Shard上
- 索引的 Mapping 与 Settings
	- Mapping 定义文档字段的类型
	- Setting 定义不同的数据分布
### 节点
#### Master-eligible nodes 和 Master Node
- 每个节点启动后，默认就是一个Master eligible节点
	- 可以设置node.master:false禁止
- Master-eligible节点可以参加选主流程，成为Master节点
- 当第一个节点启动时候，它会将自己选举成Master节点
- 每个节点上都保存了集群的状态，只有Master节点才能修改集群的状态信息
	- 集群状态（Cluster State),维护了一个集群中，必要的信息
		- 所有的节点信息
		- 所有的索引和其相关的Mapping与Setting信息
		- 分片的路由信息
#### Data Node & Coordinating Node

Data Node
- 可以保存数据的节点，叫做Data Node。负责保存分片数据。在数据扩展上起到了
  至关重要的作用
  
  Coordinating Node
- 负责接受Client的请求，将请求分发到合适的节点，最终把结果汇集到一起
- 每个节点默认都起到了Coordinating Node的职责
#### Hot & Warm Node

不同硬件配置的Data Node,用来实现Hot&Warm架构，降低集群部署的成本
#### Machine Learning Node

负责跑机器学习的Job,用来做异常检测
#### Tribe Node

Tribe Node连接到不同的Elasticsearch集群，并且支持将这些集群当成一个单独的集群处理5.3开始使用Cross Cluster Serarch)
### 分片
- 主分片(Primary Shard )，用以解决数据水平扩展的问题。通过主分片，可以将数据分布到集群内的所有节点之上
	- 一个分片是一个运行的Lucene的实例
	- 主分片数在索引创建时指定，后续不允许修改，除非Reindex
- ·副本(Replica Shard)，用以解决数据高可用的问题。分片是主分片的拷贝
	- 副本分片数，可以动态题调整
	- 增加副本数，还可以在一定程度上提高服务的可用性（读取的吞吐）
- 对于生产环境中分片的设定，需要提前做好容量规划
	- 分片数设置过小
		- 导致后续无法增加节点实现水品扩展
		- 单个分片的数据量太大，导致数据重新分配耗时
	- 分片数设置过大，7.0开始，默认主分片设置成1,解决了over-sharding的问题
		- 影响搜索结果的相关性打分，影响统计结果的准确性
		- 单个节点上过多的分片，会导致资源浪费，同时也会影响性能
### 倒排索引
- 单词词典（Term Dictionary),记录所有文档的单词，记录单词到倒排列表的关联关系
	- 单词词典一般比较大，可以通过B+树或哈希拉链法实现，以满足高性能的插入与查询
- 倒排列表（Posting List)-记录了单词对应的文档结合，由倒排索引项组成
	- 倒排索引项（Posting)
		- 文档ID
		- 词频TF-该单词在文档中出现的次数，用于相关性评分
		- 位置（Position)-单词在文档中分词的位置。用于语句搜索（phrase query
		- 偏移（Offset)-记录单词的开始结束位置，实现高亮显示
		  
		    ![image-20220504182115226](https://cdn.jsdelivr.net/gh/GayHub1/images@master/img/image-20220504182115226.png)
- ### 分词器
  
  常见分词器
  
  | 名称                                                         | 作用                                                         |
  | ------------------------------------------------------------ | ------------------------------------------------------------ |
  | Simple Analyzer                                              | 按照非字母切分（符号被过滤），小写处理                       |
  | Stop Analyzer                                                | 小写处理，停用词过滤（the，a，is）                           |
  | Whitespace Analyzer                                          | 按照空格切分，不转小写                                       |
  | Patter Analyze                                               | 正则表达式，默认 \W+ (非字符分隔)                            |
  | Keyword Analyzer                                             | 不分词，直接将输入当作输出                                   |
  | Language                                                     | 提供了30多种常见语言的分词器                                 |
  | [IK]( https://github.com/medcl/elasticsearch-analysis-ik)    | 支持自定义词库，支持热更新分词字典                           |
  | [THULAC](https://github.com/microbun/elasticsearch-thulac-plugin) | THU Lexucal Analyzer for Chinese,清华大学自然语言处理和社会人文计算实验室的一套中文分词器 |
  
  **安装**
  
  1. 将[IK分词器压缩包]( https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.1.0/elasticsearch-analysis-ik-7.1.0.zip)映射进容器内/usr/share/elasticsearch/plugins目录下，修改在docker-compose文件，在**volumes**下添加
   ```
         - ./ELKConfig/elasticsearch/plugins:/usr/share/elasticsearch/plugins #插件文件挂载
   ```
  2. 也可以容器内命令安装
   ```
   ./bin/elasticsearch-plugin install  https://github.com/medcl/elasticsearch-analysis-ik/releases/downlo
   ad/v7.1.0/elasticsearch-analysis-ik-7.1.0.zip
   ```
  3. 重启所有es容器
  4. 访问http://127.0.0.1:9200/_cat/plugins 查看插件是否被加载进去
   ![image-20220504213020492](C:\Users\94241\AppData\Roaming\Typora\typora-user-images\image-20220504213020492.png)
  测试
  ```
  POST _analyze
  {
  "analyzer": "ik_smart",
  "text": "这个苹果不大好吃"
  }
  ```
  
  如果安装插件后重启容器失败，可以将插件压缩包解压到plugins文件夹下ik文件夹
### Search API

指定索引

![image-20220504214925983](https://cdn.jsdelivr.net/gh/GayHub1/images@master/img/image-20220504214925983.png)
#### URL Search

![image-20220518233406287](C:\Users\94241\AppData\Roaming\Typora\typora-user-images\image-20220518233406287.png)
- q:  指定查询语句，使用Query String Syntax
- df: 默认字段，不指定时，会对所有字段进行查询
- Sort 排序/from和size用于分页
- Profile 可以查看查询是如何被执行的
- Term V.S Phrase
	- Beautiful Mind 等效于 Beautiful OR  Mind    title:(Beautiful AND Mind)
	- “Beautiful Mind”，等效于 Beautiful AND Mind。Phrase查询，还要求前后顺序保持一致 title="Beautiful Mind"
- 布尔操作
	- AND/OR/NOT或者&&/ll/!
	- 必须大写
		- title:(matrix NOT reloaded)
- 分组
	- \+ 表示must
	- \- 表示must_not
	- title:(+matrix -reloaded)
- 范围查询
	- 区间表示：[] 闭区间 ，{} 开区间
		- year:{2019 TO 2018}
		- year:[* TO 2018]
- 算数符号
	- year:&gt;2010
	- year:(>2010 && &lt;=2018)
	- year:(+>2010 +<=2018)
- 通配符查询（通配符查询效率低，占用内存大，不建议使用。特别是放在最前面）
	- ？代表1个字符，* 代表 0 或 多个字符
		- title:mi?d
		- title:be*
- ·正则表达
	- title:[bt]oy
- 模糊匹配与近似查询
	- title:befutifl~1
	- title:"lord rings"~2
#### Request Body Search

![image-20220518232842975](https://cdn.jsdelivr.net/gh/GayHub1/images@master/img/image-20220518232842975.png)
## 查询语句

```
###获取节点信息列表
GET /_cat/nodes?v

GET /_cat/nodes?v&h=id,ip,port,v,m
###获取对应节点详细信息
GET /_nodes/elasticsearch1,elasticsearch2


###获取节点健康状态
GET _cluster/health
GET _cluster/health?level=shards
GET /_cluster/health/movies,.kibana_1
GET /_cluster/health/movies?level=shards

#### 集群状态
The cluster state API allows access to metadata representing the state of the whole cluster. This includes information such as
GET /_cluster/state

#集群设置
GET /_cluster/settings
GET /_cluster/settings?include_defaults=true
#分片状态
GET _cat/shards
GET _cat/shards?h=index,shard,prirep,state,unassigned.reason

```
### CRUD

```
############创建文档############
#create document. 自动生成 _id
POST users/_doc
{
	"user" : "Mike",
  "post_date" : "2019-04-15T14:12:12",
  "message" : "trying out Kibana"
}

#create document. 指定Id。如果id已经存在，报错
PUT users/_doc/1?op_type=create
{
  "user" : "Jack",
  "post_date" : "2019-05-15T14:12:12",
  "message" : "trying out Elasticsearch"
}

#create document. 指定 ID 如果已经存在，就报错
PUT users/_create/1
{
   "user" : "Jack",
  "post_date" : "2019-05-15T14:12:12",
  "message" : "trying out Elasticsearch"
}

### Get Document by ID
#Get the document by ID
GET users/_doc/1


###  Index & Update
#Update 指定 ID  (先删除，在写入)
GET users/_doc/1

PUT users/_doc/1
{
	"user" : "Mike"

}


#GET users/_doc/1
#在原文档上增加字段
POST users/_update/1/
{
  "doc":{
      "post_date" : "2019-05-15T14:12:12",
      "message" : "trying out Elasticsearch"
  }
}



### Delete by Id
# 删除文档
DELETE users/_doc/1


### Bulk 操作
#执行两次，查看每次的结果

#执行第1次
POST _bulk
{ "index" : { "_index" : "test", "_id" : "1" } }
{ "field1" : "value1" }
{ "delete" : { "_index" : "test", "_id" : "2" } }
{ "create" : { "_index" : "test2", "_id" : "3" } }
{ "field1" : "value3" }
{ "update" : {"_id" : "1", "_index" : "test"} }
{ "doc" : {"field2" : "value2"} }


#执行第2次
POST _bulk
{ "index" : { "_index" : "test", "_id" : "1" } }
{ "field1" : "value1" }
{ "delete" : { "_index" : "test", "_id" : "2" } }
{ "create" : { "_index" : "test2", "_id" : "3" } }
{ "field1" : "value3" }
{ "update" : {"_id" : "1", "_index" : "test"} }
{ "doc" : {"field2" : "value2"} }

### mget 操作
GET /_mget
{
  "docs" : [
      {
          "_index" : "test",
          "_id" : "1"
      },
      {
          "_index" : "test",
          "_id" : "2"
      }
  ]
}


#URI中指定index
GET /test/_mget
{
  "docs" : [
      {

          "_id" : "1"
      },
      {

          "_id" : "2"
      }
  ]
}


GET /_mget
{
  "docs" : [
      {
          "_index" : "test",
          "_id" : "1",
          "_source" : false
      },
      {
          "_index" : "test",
          "_id" : "2",
          "_source" : ["field3", "field4"]
      },
      {
          "_index" : "test",
          "_id" : "3",
          "_source" : {
              "include": ["user"],
              "exclude": ["user.location"]
          }
      }
  ]
}

### msearch 操作
POST kibana_sample_data_ecommerce/_msearch
{}
{"query" : {"match_all" : {}},"size":1}
{"index" : "kibana_sample_data_flights"}
{"query" : {"match_all" : {}},"size":2}


### 清除测试数据
#清除数据
DELETE users
DELETE test
DELETE test2
```
-