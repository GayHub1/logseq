- #[[excerpt]] [延时队列我在项目里是怎么实现的？ - SegmentFault 思否](https://segmentfault.com/a/1190000041941550)
- tags: #[[SimpRead]] #[[MQ]] #[[延迟消息]]
- read date: [[2022_06_06  ]]
- desc: 我看到这个问题之后，稍微思考了下，觉得确实也是 austin 平台所需要实现的功能。对于前端而言，只要让业务方在创建模板的时候填选屏蔽类型，后端根据这个字段...
- [📌](<http://localhost:7026/reading/5?title=延时队列我在项目里是怎么实现的？ - SegmentFault 思否#id=1654506226374>)  延迟消息如何实现？
	- [📌](<http://localhost:7026/reading/5?title=延时队列我在项目里是怎么实现的？ - SegmentFault 思否#id=1654506240047>)  原生的 Java 有`DelayQueue`
	  id:: de8f36e3-3138-4590-b6c5-bc83a58b9735
		- [📌](<http://localhost:7026/reading/5?title=延时队列我在项目里是怎么实现的？ - SegmentFault 思否#id=1654506283512>)  **没有持久化**的，重启就会导致数据丢失
	- [📌](<http://localhost:7026/reading/5?title=延时队列我在项目里是怎么实现的？ - SegmentFault 思否#id=1654506302496>)  将需要延迟的消息放置 Redis，通过`Timer`轮询得到可执行的消息，将可执行的消息放置不同的`Topic`供业务方自行消费。
	  id:: 5afde44b-7e2e-456e-b23c-574cd0623862
	- [📌](<http://localhost:7026/reading/5?title=延时队列我在项目里是怎么实现的？ - SegmentFault 思否#id=1654506314402>)  通过 Redis 的**过期回调**的姿势也能达到延迟消息的效果（把消息执行的时间定义为`key`过期的时间，当`key`触发了过期回调，那说明该消息可执行了 [[Springboot集成Redis过期回调]]
	  id:: 235cc110-e11d-43f9-a70f-9c0b046d35da
	  对Redis性能有所影响
	-
	- [📌](<http://localhost:7026/reading/5?title=延时队列我在项目里是怎么实现的？ - SegmentFault 思否#id=1654506376943>)  `RabbmitMQ`它的延迟队列机制本质上也是通过`TTL`（Time To Live 消息存活的时间）所实现的，当队列里的元素触发了过期时，会被送往到`Dead Letter Exchanges`（死信队列中)。我们可以将死信队列的元素再次转发，对其进行消费，从而达到延迟队列的效果。
	  id:: a9d7f098-e7e1-4186-9faa-87816c19e449
	- [📌](<http://localhost:7026/reading/5?title=延时队列我在项目里是怎么实现的？ - SegmentFault 思否#id=1654506389609>)  `RocketMQ`支持在我们投递消息的时候设置**延迟等级**
		- [📌](<http://localhost:7026/reading/5?title=延时队列我在项目里是怎么实现的？ - SegmentFault 思否#id=1654507013701>)  当我们设置了延迟等级的消息之后，`RocketMQ`不会把消息直接投递到对应的 topic，而是**转发到**对应延迟等级的队列中。在 Broker 内部会为每个延迟队列起`TimerTask`来进行判断是否有消息到达了时间。
	- [📌](<http://localhost:7026/reading/5?title=延时队列我在项目里是怎么实现的？ - SegmentFault 思否#id=1654507033644>)  **万物皆扫表**
	  id:: 9833e0dc-26f0-4e6a-89ff-3ab5fd4bdba7
	  
	  针对这次需求（晚上发的消息，次日早上发送），就不需要上延时队列，因为`austin`已经接入了分布式定时任务框架了（对应的实现是`xxl-job`）
	  
	  只要把晚上的接收到的消息扔进`Redis list`，然后启个定时任务（每天早上 9 点）轮询该`list`是否有数据，如果有再重新做处理就完事了。